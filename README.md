# Transformer and iTransformer

This repository presents an analytical study, implementing the **Transformer** and **iTransformer** architectures from scratch comparing for sequence modeling tasks.

### Overview
The project explores the evolution of transformer-based models, emphasizing improvements introduced in the **iTransformer** framework over the standard Transformer. It examines their attention mechanisms, positional encodings, and computational efficiency, highlighting the design motivations behind the iTransformer.
The study demonstrates that iTransformer achieves competitive or superior performance with improved efficiency in sequence learning tasks, validating its architectural refinements over the baseline Transformer.

### References
Refer to the PDF report for full citations and related work.

## Author
**Sina Heydari**  
Winter 2024  
Computer Scientist & AI Researcher
